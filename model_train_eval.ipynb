{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Foreword:</b><br>\n",
    "This notebook shows an example of how the model is used on a sample dataset.\n",
    "\n",
    "This sample dataset (~31,000 entries) is taken from <b>UCI Machine Learning Repository</b>. \n",
    "For the purposes of this example, the task will be to predict whether an individual earns above or below 50,000 annually using the Decision Tree/Random Forest models built from scratch. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "#import required libraries \n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import numpy as np\n",
    "from sklearn import metrics\n",
    "from src.datapipeline import transform\n",
    "from src.decision_tree import DecisionTree\n",
    "from src.random_forest import RandomForest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we need to clean the raw data. For purposes of illustration, I have written a data pipeline that does the cleaning and returns a train test split.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\rosam\\OneDrive\\Desktop\\RF_from_scratch\\src\\datapipeline.py:69: FutureWarning: Downcasting behavior in `replace` is deprecated and will be removed in a future version. To retain the old behavior, explicitly call `result.infer_objects(copy=False)`. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n",
      "  df['education'] = df['education'].replace(value_to_index)\n"
     ]
    }
   ],
   "source": [
    "# load data \n",
    "data_path = './data/data.csv'\n",
    "X_train, X_test, y_train, y_test = transform(data_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's inspect the shape of the cleaned data. \n",
    "We have 20,734 entries for our train set; 10,213 entries for our test set.\n",
    "We also have 294 columns which represents features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train shape: (20734, 294)\n",
      "X_test shape: (10213, 294)\n",
      "y_train shape: (20734, 1)\n",
      "y_test shape: (10213, 1)\n"
     ]
    }
   ],
   "source": [
    "print('X_train shape:', X_train.shape)\n",
    "print('X_test shape:', X_test.shape)\n",
    "print('y_train shape:',y_train.shape)\n",
    "print('y_test shape:', y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that our data transformation pipeline creates dataframes. \n",
    "So we need to convert them into numpy arrays first. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.values\n",
    "X_test = X_test.values\n",
    "y_train = y_train.values\n",
    "y_test = y_test.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <b>Part 1: Decision Tree</b>: \n",
    "Now we fit the decision tree on our train dataset.\n",
    "\n",
    "A decision tree is a flexible, non-parametric model used for both classification and regression. In my implementation, I built a tree-like structure by splitting the data into smaller groups based on feature values. At each step, the tree chooses the feature and threshold that result in the best split, by reducing Gini Impurity. This process continues until a stopping condition is met, such as reaching a maximum depth or having too few samples to split further."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = DecisionTree(max_depth=3)\n",
    "fitted_tree = dt.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Model Inference and Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We completed fitting the decision tree! Now lets evaluate it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_dt = dt.predict(X_test, fitted_tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The predicted y is a list of 0s and 1, indicating the class prediction. \n",
    "Whereas the true y has the shape (10213, 1). This means we need to apply np.squeeze in order to remove one dimension. This allows us to apply sklearn's classification report to evaluate the performance of the decision tree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted y shape: (10213,)\n",
      "true y shape: (10213, 1)\n"
     ]
    }
   ],
   "source": [
    "print(\"predicted y shape:\", pred_dt.shape)\n",
    "print(\"true y shape:\", y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performance metrics for Decision Tree from scratch:\n",
      "-----------------------------------------------------\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.89      0.79      0.84      6128\n",
      "           1       0.73      0.86      0.79      4085\n",
      "\n",
      "    accuracy                           0.82     10213\n",
      "   macro avg       0.81      0.83      0.82     10213\n",
      "weighted avg       0.83      0.82      0.82     10213\n",
      "\n"
     ]
    }
   ],
   "source": [
    "report = metrics.classification_report(np.squeeze(y_test), pred_dt)\n",
    "print('Performance metrics for Decision Tree from scratch:\\n-----------------------------------------------------\\n', report)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <b>Part 2. Random Forest </b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random Forests are an ensemble learning method for classification, regression and other tasks that works by creating a multitude of decision trees during training.\n",
    "\n",
    "The Random Forest I built from scratch using only Numpy is for the purposes of classification task. <br>\n",
    "\n",
    "To create a Random Forest, we need to define the number of Decision Trees in the forest.I draw samples from the X_train set using bootstrap aggregation (just a fancy term for sampling with replacement, also called bagging); I also sample features from the X_train set. How many times do I draw? As many times as the number of trees in my random forest. <br>\n",
    "\n",
    "Model training happens by fitting Decision Trees to each sample data. <br>\n",
    "\n",
    "For inferencing, (we use the X_test data now) we use a simple averaging mechanism, where the \"votes\" are averaged across the trees for each data point (an entry of X_test)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf = RandomForest(n_trees=100, subsample_size=0.5, feature_proportion=0.5)\n",
    "rf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Model Inferencing and Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rmb that we use X_test set for inferencing!\n",
    "We can just call the predict method here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds_rf = rf.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see from the classification report below, the random forest that I have built from scratch scored an accuracy of 88 %. As expected, this is an improvement of +6 percentage points (7.3%) from using a single decision tree (82 % accuracy). \n",
    "\n",
    "Accuracy is a fair metric to use here because the dataset is not too imbalanced; you can observe our precision and recall scores are not too far off from each other too. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performance metrics for Random Forest from scratch:\n",
      "-----------------------------------------------------\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           0       0.91      0.88      0.90      6128\n",
      "           1       0.83      0.87      0.85      4085\n",
      "\n",
      "    accuracy                           0.88     10213\n",
      "   macro avg       0.87      0.88      0.87     10213\n",
      "weighted avg       0.88      0.88      0.88     10213\n",
      "\n"
     ]
    }
   ],
   "source": [
    "report = metrics.classification_report(np.squeeze(y_test), np.squeeze(preds_rf))\n",
    "\n",
    "print('Performance metrics for Random Forest from scratch:\\n-----------------------------------------------------\\n', report)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rf_from_scratch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
